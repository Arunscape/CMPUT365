 In many problems the NN used to approximate the value function is much large than the task to be solved (e.g., we use networks with more than a dozen layers and millions of parameters to learn in atari; but the memory size in Atari is only 128 bytes). This is the over-parameterized regime. If we build a robot to learn and interact with humans will we still be in the over-parameterized regime? Will our Deep NNs be bigger than the world?


[12]I think it is likely that neural networks in the future will be over-parametrized when learning and interacting with humans. In 2013 it was possible to train 11 billion parameter networks with a scalable system.(https://www.theregister.com/2013/06/18/google_deep_learning_ai/) The human brain consists of approximately 100 billion neurons.(https://www.newworldencyclopedia.org/entry/Neuron) It is reasonable to assume that in the future, it will be possible to have more parameters in artificial neural networks than neurons in the human brain. A regime where there are more parameters than the memory size of the problem is an over-parameterized regime. For example, a deep neural network with dozens of layers and millions of parameters solving a 128 byte Atari game. Such a model would be able to memorize the game and even solve more complex tasks.

There is more to intelligence than just the number of neurons in a network. Synapses, or connections between neurons, are the units responsible for computation. The human brain contains approximately 100 trillion synapses. It may seem near impossible for a computer to simulate trillions of connections, however, supercomputers nowadays can do over 10,000,000,000,000,000 calculations per second.(https://www.japantimes.co.jp/life/2012/02/12/general/10000000000000000-calculations-per-second/) Right now, I do not think we would be able to build a useful over-parametrized regime for complicated tasks like learning and interacting with humans. We may be able to build an over-parametrized model for such tasks right now. The problem would be simulating enough connections between the neurons. Without enough connections, the model would likely not be useful.  A neural network with fewer parameters but more connections can outperform one with many parameters and few connections. Deep neural networks could easily be bigger than the world with time. More complicated robots that can learn and interact with humans will be built as technology progresses. These useful over-parametrized models will run on computers much faster than the ones today.
